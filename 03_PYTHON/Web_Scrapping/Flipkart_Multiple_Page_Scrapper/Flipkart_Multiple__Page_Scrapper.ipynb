{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87eea525-51c1-452b-a45e-316d77b86ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1: 200\n",
      "Scraping page 2: 200\n",
      "Scraping page 3: 200\n",
      "Scraping page 4: 200\n",
      "Scraping page 5: 200\n",
      "Scraping page 6: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 6: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 6: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 6: 200\n",
      "Scraping page 7: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 7: 200\n",
      "Scraping page 8: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 8: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 8: 200\n",
      "Scraping page 9: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 9: 200\n",
      "Scraping page 10: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 10: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 10: 200\n",
      "Scraping page 11: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 11: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 11: 200\n",
      "Scraping page 12: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 12: 200\n",
      "Scraping page 13: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 13: 200\n",
      "Scraping page 14: 200\n",
      "Scraping page 15: 200\n",
      "Scraping page 16: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 16: 200\n",
      "Scraping page 17: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 17: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 17: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 17: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Scraping page 17: 429\n",
      "Rate limit hit. Waiting before retrying...\n",
      "Rate limit not overcome. Exiting the loop.\n",
      "Length of Product_name: 360\n",
      "Length of Prices: 359\n",
      "Length of Description: 360\n",
      "Length of Reviews: 348\n",
      "Data scraping completed and saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd  # Import pandas for data manipulation and analysis\n",
    "import requests  # Import requests for sending HTTP requests\n",
    "from bs4 import BeautifulSoup  # Import BeautifulSoup from bs4 for parsing HTML and XML documents\n",
    "import csv  # Import csv for handling CSV files\n",
    "import time  # Import time to add delay between requests to avoid rate limiting\n",
    "import random  # Import random to generate random sleep times between requests\n",
    "\n",
    "# Define headers to mimic a browser request to avoid being blocked by the website\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Initialize lists outside the loop to accumulate data across pages\n",
    "Product_name = []  # List to store product names\n",
    "Prices = []  # List to store product prices\n",
    "Description = []  # List to store product descriptions\n",
    "Reviews = []  # List to store product reviews\n",
    "\n",
    "# Loop through 1 to 99 to scrape data from multiple pages\n",
    "for i in range(1, 150):\n",
    "    # Construct the URL with the page number i\n",
    "    url = f\"https://www.flipkart.com/search?q=mobiles+under+50000&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&as-pos=1&as-type=HISTORY&page={i}\"\n",
    "    \n",
    "    # Attempt to fetch the page with retries to handle potential rate limits or errors\n",
    "    for attempt in range(5):\n",
    "        page = requests.get(url, headers=headers)  # Send a GET request to the specified URL with headers\n",
    "        print(f\"Scraping page {i}: {page.status_code}\")  # Print the status code to know the result of the request\n",
    "        \n",
    "        if page.status_code == 429:\n",
    "            # If the status code is 429 (Too Many Requests), wait before retrying\n",
    "            print(\"Rate limit hit. Waiting before retrying...\")\n",
    "            time.sleep(10 + attempt * 5)  # Exponential backoff: wait longer with each attempt\n",
    "            continue  # Retry the request\n",
    "        elif page.status_code == 200:\n",
    "            # If the status code is 200 (OK), the request was successful\n",
    "            break  # Exit the retry loop\n",
    "        else:\n",
    "            # If the status code is not 200 or 429, handle it as an unexpected status\n",
    "            print(f\"Unexpected status code {page.status_code}. Stopping the loop.\")\n",
    "            break  # Exit both loops\n",
    "      # If the status is still 429 after retries, stop the entire scraping process\n",
    "    if page.status_code == 429:\n",
    "        print(\"Rate limit not overcome. Exiting the loop.\")\n",
    "        break  # Exit the main loop\n",
    "    \n",
    "    # Get the content of the page from the response\n",
    "    fd = page.content\n",
    "    \n",
    "    # Parse the HTML content of the page with BeautifulSoup\n",
    "    soup = BeautifulSoup(fd, 'html.parser')\n",
    "    \n",
    "    # Find the main container that holds the product listings\n",
    "    box = soup.find('div', class_='DOjaWF gdgoEp')\n",
    "    \n",
    "    # Check if the box is None (i.e., no products found on this page)\n",
    "    if box is None:\n",
    "        print(f\"No products found on page {i}. Stopping the loop.\")\n",
    "        break  # Stop scraping further pages if no products are found\n",
    "    \n",
    "    # Finding and storing Product Names\n",
    "    names = box.find_all('div', class_='KzDlHZ')  # Find all elements with the product name class\n",
    "    for name in names:\n",
    "        Product_name.append(name.text)  # Append the text content of each name to the Product_name list\n",
    "    \n",
    "    # Finding and storing Product Prices\n",
    "    prices = box.find_all('div', class_='Nx9bqj _4b5DiR')  # Find all elements with the product price class\n",
    "    for price in prices:\n",
    "        Prices.append(price.text.strip())  # Append the cleaned (stripped) text of each price to the Prices list\n",
    "\n",
    "    # Finding and storing Product Descriptions\n",
    "    desc = box.find_all('ul', class_='G4BRas')  # Find all elements with the product description class\n",
    "    for description in desc:\n",
    "        Description.append(description.text)  # Append the text content of each description to the Description list\n",
    "    \n",
    "    # Finding and storing Product Reviews\n",
    "    reviews = box.find_all('div', class_='XQDdHH')  # Find all elements with the product review class\n",
    "    for review in reviews:\n",
    "        Reviews.append(review.text)  # Append the text content of each review to the Reviews list\n",
    "\n",
    "    # Random sleep time between requests to avoid getting blocked by the website's rate limiting\n",
    "    time.sleep(random.uniform(2, 5))  # Sleep for a random duration between 2 and 5 seconds\n",
    "\n",
    "# After scraping all pages, ensure data consistency\n",
    "\n",
    "# Check the lengths of the lists to verify that they are equal\n",
    "print(f\"Length of Product_name: {len(Product_name)}\")\n",
    "print(f\"Length of Prices: {len(Prices)}\")\n",
    "print(f\"Length of Description: {len(Description)}\")\n",
    "print(f\"Length of Reviews: {len(Reviews)}\")\n",
    "\n",
    "# Ensure all lists have the same length by truncating them to the length of the shortest list\n",
    "min_length = min(len(Product_name), len(Prices), len(Description), len(Reviews))\n",
    "\n",
    "Product_name = Product_name[:min_length]  # Truncate the Product_name list to the minimum length\n",
    "Prices = Prices[:min_length]  # Truncate the Prices list to the minimum length\n",
    "Description = Description[:min_length]  # Truncate the Description list to the minimum length\n",
    "Reviews = Reviews[:min_length]  # Truncate the Reviews list to the minimum length\n",
    "\n",
    "# Create a DataFrame to organize the data\n",
    "df = pd.DataFrame({\n",
    "    'Product Name': Product_name,  # Add the Product_name list as a column named 'Product Name'\n",
    "    'Prices': Prices,  # Prepend 'â‚¹' to each price and add it as a column named 'Prices'\n",
    "    'Description': Description,  # Add the Description list as a column named 'Description'\n",
    "    'Reviews': Reviews  # Add the Reviews list as a column named 'Reviews'\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"E:/#DATA_ANALYST_PORTFOLIO_PROJECTS/03_PYTHON/Web_Scrapping/Flipkart_Multiple_Page_Scrapper/Flipkart_mobiles_under_50000.csv\", \n",
    "          index=False, encoding='utf-8-sig')  # Save the DataFrame to a CSV file without the index and with UTF-8 encoding\n",
    "\n",
    "print(\"Data scraping completed and saved to CSV.\")  # Indicate that the scraping and saving process is complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd3b2df-6490-4b57-ab43-dfddf1e115cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
